# cctv_event_detector/inference/strategies/object_boundary_segmenter.py
import torch
import numpy as np
import cv2
from typing import Dict, Any, List

# --- ì„¤ì • ë° ê¸°ë³¸ í´ë˜ìŠ¤ ì„í¬íŠ¸ ---
from config import (
    SAM_CHECKPOINT_PATH,
    SAM_MODEL_TYPE,
    BOUNDARY_TARGET_CLASSES,
    SAM_BOUNDARY_OUTPUT_DIR,
)
from .base import InferenceStrategy
from cctv_event_detector.core.models import CapturedImage

# --- SAM ê´€ë ¨ í´ë˜ìŠ¤ ì„í¬íŠ¸ ---
from segment_anything import sam_model_registry, SamPredictor

class SAMObjectBoundarySegmenter(InferenceStrategy):
    """
    YOLOë¡œ íƒì§€ëœ ê°ì²´ì˜ BBoxë¥¼ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©í•˜ì—¬ SAMìœ¼ë¡œ ì •í™•í•œ ê²½ê³„ì„ ì„ ì°¾ëŠ” ì „ëµ í´ë˜ìŠ¤ì…ë‹ˆë‹¤.
    """
    def __init__(self):
        """
        SAM ëª¨ë¸ê³¼ Predictorë¥¼ ì´ˆê¸°í™”í•˜ê³ , configì—ì„œ ì„¤ì •ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
        """
        print("Initializing SAM Object Boundary Segmenter...")
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # SAM ëª¨ë¸ ë¡œë“œ
        sam = sam_model_registry[SAM_MODEL_TYPE](checkpoint=str(SAM_CHECKPOINT_PATH))
        sam.to(device=self.device)
        self.predictor = SamPredictor(sam)
        print(f"âœ… SAM Predictor ('{SAM_MODEL_TYPE}') loaded successfully.")

        # âœ¨ ìš”êµ¬ì‚¬í•­ 2: íƒ€ê²Ÿ í´ë˜ìŠ¤ ì„¤ì •ì´ ì—†ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ë¹ˆ setìœ¼ë¡œ ì²˜ë¦¬
        self.target_classes = set(BOUNDARY_TARGET_CLASSES) if BOUNDARY_TARGET_CLASSES else set()
        
        if self.target_classes:
            print(f"ğŸ¯ Target classes for boundary finding: {self.target_classes}")
        else:
            print("ğŸ¯ Target classes not specified, processing all classes.")

        self.output_dir = SAM_BOUNDARY_OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ’¾ Boundary visualization output dir: {self.output_dir}")

    def run(self, captured_images: List[CapturedImage], inference_results: Dict[str, Any]) -> Dict[str, Any]:
        print("Running SAM Object Boundary Segmenter...")

        for capture in captured_images:
            image_id = capture.image_id
            original_image = capture.image_data

            detections = inference_results.get(image_id, {}).get("detections", [])
            if not detections:
                continue

            rgb_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)
            self.predictor.set_image(rgb_image)

            boundary_masks = []
            vis_image = original_image.copy()
            
            # âœ¨ ìš”êµ¬ì‚¬í•­ 1: ì‹œê°í™” ê°•í™”ë¥¼ ìœ„í•´ ëª¨ë“  YOLO íƒì§€ ê²°ê³¼ë¥¼ ë¨¼ì € ê·¸ë¦¼
            for detection in detections:
                self._draw_yolo_bbox_and_label(vis_image, detection)

            for detection in detections:
                # âœ¨ ìš”êµ¬ì‚¬í•­ 2: íƒ€ê²Ÿ í´ë˜ìŠ¤ê°€ ë¹„ì–´ìˆê±°ë‚˜, í´ë˜ìŠ¤ê°€ íƒ€ê²Ÿì— í¬í•¨ë˜ë©´ ì²˜ë¦¬
                should_process = not self.target_classes or detection['class_name'] in self.target_classes
                
                if should_process:
                    x, y, w, h = detection['bbox_xywh']
                    input_box = np.array([x - w/2, y - h/2, x + w/2, y + h/2])

                    masks, scores, _ = self.predictor.predict(
                        box=input_box,
                        multimask_output=False
                    )
                    mask = masks[0]
                    
                    boundary_masks.append({
                        "class_name": detection['class_name'],
                        "confidence": scores[0],
                        "segmentation_mask": mask
                    })
                    # SAM ë§ˆìŠ¤í¬ëŠ” ì²˜ë¦¬ëœ ê°ì²´ì— ëŒ€í•´ì„œë§Œ ê·¸ë¦¼
                    self._draw_sam_mask_on_image(vis_image, mask)

            if boundary_masks:
                # output_path = self.output_dir / image_id / f"{image_id}_boundaries.jpg"
                output_dir_path = self.output_dir / image_id
                output_dir_path.mkdir(parents=True, exist_ok=True)
                output_path = output_dir_path / f"{image_id}_boundaries.jpg"
                cv2.imwrite(str(output_path), vis_image)
                print(f"ğŸ–¼ï¸ Saved boundary visualization to {output_path}")

                if "boundary_masks" not in inference_results[image_id]:
                    inference_results[image_id]['boundary_masks'] = []
                inference_results[image_id]['boundary_masks'].extend(boundary_masks)

        return inference_results

    def _draw_yolo_bbox_and_label(self, image: np.ndarray, detection: Dict[str, Any]):
        """
        ì´ë¯¸ì§€ì— YOLO ë°”ìš´ë”© ë°•ìŠ¤ì™€ í´ë˜ìŠ¤ ë ˆì´ë¸”ì„ ê·¸ë¦½ë‹ˆë‹¤.
        """
        x, y, w, h = detection['bbox_xywh']
        class_name = detection['class_name']
        
        # BBox ì¢Œí‘œ ê³„ì‚°
        x1, y1 = int(x - w / 2), int(y - h / 2)
        x2, y2 = int(x + w / 2), int(y + h / 2)
        
        # BBox ê·¸ë¦¬ê¸° (ì´ˆë¡ìƒ‰)
        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # í´ë˜ìŠ¤ëª… ë° ì‹ ë¢°ë„ í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        label = f"{class_name}"
        (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
        
        # í…ìŠ¤íŠ¸ ë°°ê²½ ë° í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
        cv2.rectangle(image, (x1, y1 - 20), (x1 + w, y1), (0, 255, 0), -1)
        cv2.putText(image, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 2)

    def _draw_sam_mask_on_image(self, image: np.ndarray, mask: np.ndarray):
        """
        ì£¼ì–´ì§„ ì´ë¯¸ì§€ì— SAM ë§ˆìŠ¤í¬ë¥¼ ëœë¤ ìƒ‰ìƒìœ¼ë¡œ ì˜¤ë²„ë ˆì´í•©ë‹ˆë‹¤.
        """
        color = np.random.randint(0, 255, 3, dtype=np.uint8)
        overlay = image.copy()
        overlay[mask] = color
        cv2.addWeighted(overlay, 0.5, image, 0.5, 0, image)