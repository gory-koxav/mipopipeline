# cctv_event_detector/inference/strategies/assembly_classifier.py
import numpy as np
import cv2
from typing import Dict, Any, List
from ultralytics import YOLO

from cctv_event_detector.inference.strategies.base import InferenceStrategy
from cctv_event_detector.core.models import CapturedImage
from config import ASSEMBLY_CLS_MODEL_PATH, BOUNDARY_TARGET_CLASSES, SAM_CLASSIFICATION_OUTPUT_DIR

class AssemblyClassifier(InferenceStrategy):
    """ì¡°ë¦½ ìƒíƒœ ë¶„ë¥˜ ëª¨ë¸ (Strategy êµ¬í˜„ì²´)"""
    
    def __init__(self):
        """
        YOLO classification ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ì„¤ì •ì„ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        print("Initializing Assembly Classifier...")
        self.cls_model = YOLO(ASSEMBLY_CLS_MODEL_PATH)
        print(f"âœ… Assembly Classification model loaded from: {ASSEMBLY_CLS_MODEL_PATH}")
        
        # BOUNDARY_TARGET_CLASSESë¥¼ ì‚¬ìš©í•˜ì—¬ ì²˜ë¦¬í•  í´ë˜ìŠ¤ ì„¤ì •
        self.target_classes = set(BOUNDARY_TARGET_CLASSES) if BOUNDARY_TARGET_CLASSES else set()
        if self.target_classes:
            print(f"ğŸ¯ Target classes for assembly classification: {self.target_classes}")
        else:
            print("ğŸ¯ No target classes specified, processing all detected objects.")
        
        # ê²°ê³¼ ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ì„¤ì •
        self.output_dir = SAM_CLASSIFICATION_OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ’¾ Assembly classification output dir: {self.output_dir}")
    
    def run(self, captured_images: List[CapturedImage], inference_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì…ë ¥ëœ ì´ë¯¸ì§€ë“¤ì— ëŒ€í•´ ì¡°ë¦½ ìƒíƒœ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        print("Running Assembly Classifier...")
        
        for capture in captured_images:
            image_id = capture.image_id
            original_image = capture.image_data
            
            # YOLO Object Detection ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            detections = inference_results.get(image_id, {}).get("detections", [])
            if not detections:
                continue
            
            # ëŒ€ìƒ í´ë˜ìŠ¤ì— í•´ë‹¹í•˜ëŠ” detectionë§Œ í•„í„°ë§
            target_detections = []
            for detection in detections:
                # target_classesê°€ ë¹„ì–´ìˆê±°ë‚˜, í•´ë‹¹ í´ë˜ìŠ¤ê°€ targetì— í¬í•¨ë˜ë©´ ì²˜ë¦¬
                should_process = not self.target_classes or detection['class_name'] in self.target_classes
                if should_process:
                    target_detections.append(detection)
            
            if not target_detections:
                print(f"No target class objects found in '{image_id}'")
                continue
            
            print(f"Processing {len(target_detections)} target objects in '{image_id}'")
            
            # ê° ëŒ€ìƒ ê°ì²´ì— ëŒ€í•´ ë¶„ë¥˜ ìˆ˜í–‰
            assembly_classifications = []
            for idx, detection in enumerate(target_detections):
                classification_result = self._classify_single_object(
                    original_image, 
                    detection, 
                    image_id, 
                    idx
                )
                if classification_result:
                    assembly_classifications.append(classification_result)
            
            # ê²°ê³¼ë¥¼ inference_resultsì— ì¶”ê°€
            if assembly_classifications:
                # ì „ì²´ ì´ë¯¸ì§€ì˜ assembly_status ê²°ì • (ê°€ì¥ ë³µì¡í•œ ìƒíƒœë¡œ ì„¤ì •)
                assembly_status = self._determine_overall_status(assembly_classifications)
                inference_results[image_id]["assembly_status"] = assembly_status
                
                # ìƒì„¸ ë¶„ë¥˜ ê²°ê³¼ë„ ì €ì¥ (í•„ìš”ì‹œ ì‚¬ìš©)
                inference_results[image_id]["assembly_classifications"] = assembly_classifications
                
                # ì‹œê°í™” ê²°ê³¼ ì €ì¥
                self._save_visualization(
                    original_image, 
                    target_detections, 
                    assembly_classifications, 
                    image_id
                )
                
                print(f"âœ… Assembly status for '{image_id}': {assembly_status}")
        
        return inference_results
    
    def _classify_single_object(self, image: np.ndarray, detection: Dict[str, Any], 
                               image_id: str, idx: int) -> Dict[str, Any]:
        """
        ë‹¨ì¼ ê°ì²´ë¥¼ í¬ë¡­í•˜ê³  ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        # BBOX ì¢Œí‘œ ì¶”ì¶œ (object_detectorëŠ” [x_min, y_min, width, height] í˜•ì‹ ì‚¬ìš©)
        x_min, y_min, width, height = detection['bbox_xywh']
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸
        img_h, img_w = image.shape[:2]
        
        # BBOX ì¢Œí‘œ ë³´ì • (ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡)
        x_min = max(0, x_min)
        y_min = max(0, y_min)
        x_max = min(img_w, x_min + width)
        y_max = min(img_h, y_min + height)
        
        # ìœ íš¨í•œ í¬ê¸°ì¸ì§€ í™•ì¸
        if x_max <= x_min or y_max <= y_min:
            print(f"âš ï¸ Invalid bbox for object {idx}: skipping")
            return None
        
        # ì´ë¯¸ì§€ í¬ë¡­
        cropped_image = image[y_min:y_max, x_min:x_max]
        
        # (512, 512) í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        resized_image = cv2.resize(cropped_image, (512, 512))
        
        # YOLO classification ëª¨ë¸ë¡œ ì¶”ë¡ 
        results = self.cls_model(resized_image, verbose=False)
        result = results[0]
        
        # Top1ê³¼ Top5 ê²°ê³¼ ì¶”ì¶œ
        top1_idx = result.probs.top1
        top1_class = self.cls_model.names[top1_idx]
        top1_conf = result.probs.top1conf.item()
        
        # Top5 í´ë˜ìŠ¤ì™€ í™•ë¥ 
        top5_indices = result.probs.top5
        top5_classes = [self.cls_model.names[int(idx)] for idx in top5_indices]
        top5_probs = result.probs.top5conf.tolist()
        
        # ê°œë³„ í¬ë¡­ ì´ë¯¸ì§€ ì €ì¥ (ì‹œê°í™” í¬í•¨)
        self._save_cropped_result(resized_image, result, image_id, idx, detection['class_name'])
        
        return {
            "object_index": idx,
            "object_class": detection['class_name'],
            "bbox": detection['bbox_xywh'],
            "top1_class": top1_class,
            "top1_confidence": top1_conf,
            "top5_classes": top5_classes,
            "top5_probs": top5_probs
        }
    
    def _determine_overall_status(self, classifications: List[Dict[str, Any]]) -> str:
        """
        ì „ì²´ ì´ë¯¸ì§€ì˜ assembly_statusë¥¼ ê²°ì •í•©ë‹ˆë‹¤.
        ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ simple_assembly ë˜ëŠ” complex_assemblyë¡œ ê²°ì •
        """
        # ì˜ˆì‹œ ë¡œì§: top1_class ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨
        # ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ì— ë§ê²Œ ìˆ˜ì • í•„ìš”
        complex_indicators = ["complex", "assembled", "combined"]
        
        for classification in classifications:
            top1_class = classification['top1_class'].lower()
            for indicator in complex_indicators:
                if indicator in top1_class:
                    return "complex_assembly"
        
        # ì—¬ëŸ¬ ê°ì²´ê°€ ìˆìœ¼ë©´ complexë¡œ ê°„ì£¼
        if len(classifications) > 1:
            return "complex_assembly"
        
        return "simple_assembly"
    
    def _save_cropped_result(self, image: np.ndarray, result, image_id: str, 
                            idx: int, original_class: str):
        """
        í¬ë¡­ëœ ì´ë¯¸ì§€ì— ë¶„ë¥˜ ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        """
        # íƒ€ì„ìŠ¤íƒ¬í”„ë³„ ë””ë ‰í† ë¦¬ ìƒì„±
        timed_output_dir = self.output_dir / image_id
        timed_output_dir.mkdir(parents=True, exist_ok=True)
        
        # ì´ë¯¸ì§€ì— ë¶„ë¥˜ ê²°ê³¼ ì–´ë…¸í…Œì´ì…˜
        annotated_image = self._annotate_image(image, result)
        
        # íŒŒì¼ëª… ìƒì„± ë° ì €ì¥
        top1_class = self.cls_model.names[result.probs.top1]
        filename = f"assembly_{original_class}_{idx:03d}_{top1_class}.png"
        output_path = timed_output_dir / filename
        
        cv2.imwrite(str(output_path), annotated_image)
        print(f"  ğŸ’¾ Saved cropped result: {output_path}")
    
    def _annotate_image(self, image: np.ndarray, result) -> np.ndarray:
        """
        ì´ë¯¸ì§€ì— Top5 ë¶„ë¥˜ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
        (mask_classifier.pyì˜ _annotate_image í•¨ìˆ˜ ì°¸ê³ )
        """
        annotated_image = image.copy()
        pos_y = 40
        
        for j, idx in enumerate(result.probs.top5):
            text = f"{j+1}. {self.cls_model.names[int(idx)]}: {result.probs.top5conf[j]:.2f}"
            # ê²€ì€ìƒ‰ ì™¸ê³½ì„ 
            cv2.putText(annotated_image, text, (20, pos_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 0, 0), 3, cv2.LINE_AA)
            # í°ìƒ‰ í…ìŠ¤íŠ¸
            cv2.putText(annotated_image, text, (20, pos_y), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 2, cv2.LINE_AA)
            pos_y += 40
        
        return annotated_image
    
    def _save_visualization(self, image: np.ndarray, detections: List[Dict[str, Any]], 
                          classifications: List[Dict[str, Any]], image_id: str):
        """
        ì „ì²´ ì´ë¯¸ì§€ì— ëŒ€í•œ ì‹œê°í™” ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        BBOXì™€ ë¶„ë¥˜ ê²°ê³¼ë¥¼ í•¨ê»˜ í‘œì‹œ
        """
        timed_output_dir = self.output_dir / image_id
        timed_output_dir.mkdir(parents=True, exist_ok=True)
        
        vis_image = image.copy()
        
        # ê° detectionê³¼ classification ê²°ê³¼ë¥¼ ì‹œê°í™”
        for detection, classification in zip(detections, classifications):
            # BBOX ê·¸ë¦¬ê¸°
            x_min, y_min, width, height = detection['bbox_xywh']
            x_max = x_min + width
            y_max = y_min + height
            
            # ìƒ‰ìƒ ê²°ì • (complexë©´ ë¹¨ê°„ìƒ‰, simpleì´ë©´ ì´ˆë¡ìƒ‰)
            if "complex" in classification['top1_class'].lower():
                color = (0, 0, 255)  # Red for complex
            else:
                color = (0, 255, 0)  # Green for simple
            
            # BBOX ê·¸ë¦¬ê¸°
            cv2.rectangle(vis_image, (x_min, y_min), (x_max, y_max), color, 2)
            
            # ë ˆì´ë¸” í…ìŠ¤íŠ¸ ìƒì„±
            label = f"{detection['class_name']}: {classification['top1_class']} ({classification['top1_confidence']:.2f})"
            
            # í…ìŠ¤íŠ¸ ë°°ê²½ ë° í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸°
            (label_w, label_h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)
            cv2.rectangle(vis_image, (x_min, y_min - 25), (x_min + label_w, y_min), color, -1)
            cv2.putText(vis_image, label, (x_min, y_min - 7), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # ì „ì²´ ì‹œê°í™” ê²°ê³¼ ì €ì¥
        output_path = timed_output_dir / f"_ASSEMBLY_OVERVIEW.jpg"
        cv2.imwrite(str(output_path), vis_image)
        print(f"âœ… Assembly visualization saved: {output_path}")


# MockAssemblyClassifierë¥¼ ì‹¤ì œ êµ¬í˜„ì²´ë¡œ êµì²´
# ë‹¤ë¥¸ íŒŒì¼ì—ì„œ MockAssemblyClassifierë¥¼ importí•˜ëŠ” ê²½ìš°ë¥¼ ìœ„í•´ ë³„ì¹­ ì œê³µ
MockAssemblyClassifier = AssemblyClassifier