# cctv_event_detector/inference/strategies/mask_classifier.py
import numpy as np
import cv2
from typing import Dict, Any, List

from .base import InferenceStrategy
from cctv_event_detector.core.models import CapturedImage
from config import SAM_YOLO_CLS_MODEL_PATH, SAM_CLASSIFICATION_OUTPUT_DIR
from ultralytics import YOLO

class MaskClassifier(InferenceStrategy):
    """
    ì…ë ¥ëœ ë§ˆìŠ¤í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê°ì²´ë¥¼ í¬ë¡­í•˜ê³ , YOLO ë¶„ë¥˜ ëª¨ë¸ë¡œ ì¶”ë¡ í•œ ë’¤,
    ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•˜ëŠ” ì „ëµì…ë‹ˆë‹¤. ì´ í´ë˜ìŠ¤ëŠ” 'ë¶„ë¥˜ ë° ì‹œê°í™”'ì˜ ì±…ì„ë§Œ ê°€ì§‘ë‹ˆë‹¤.
    """
    def __init__(self):
        print("Initializing Mask Classifier...")
        self.cls_model = YOLO(SAM_YOLO_CLS_MODEL_PATH)
        print(f"âœ… YOLO classification model loaded successfully.")

        self.output_dir = SAM_CLASSIFICATION_OUTPUT_DIR
        self.output_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ’¾ Classification output dir: {self.output_dir}")

    def run(self, captured_images: List[CapturedImage], inference_results: Dict[str, Any]) -> Dict[str, Any]:
        print("Running Mask Classifier...")

        for capture in captured_images:
            image_id = capture.image_id
            original_image = capture.image_data

            masks = inference_results.get(image_id, {}).get("pinjig_masks", [])

            if masks:
                classification_results = self._perform_classification(original_image, masks, image_id)
                
                if classification_results:
                    inference_results[image_id]['pinjig_classifications'] = classification_results
                    self._visualize_results(original_image, masks, classification_results, image_id)

        return inference_results

    def _perform_classification(self, original_image: np.ndarray, masks: List[Dict[str, Any]], image_id: str) -> List[Dict[str, Any]]:
        timed_output_dir = self.output_dir / image_id
        timed_output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"Classifying {len(masks)} masks for '{image_id}'...")

        classifications = []
        for i, mask_ann in enumerate(masks):
            bbox = mask_ann.get('bbox')
            if not bbox: continue
            
            # --- ğŸ› ì—ëŸ¬ í•´ê²° 1: bbox ì¢Œí‘œë¥¼ ì •ìˆ˜í˜•ìœ¼ë¡œ ë³€í™˜ ---
            # segmentation ëª¨ë¸ì˜ ê²°ê³¼ê°’ì¸ floatë¥¼ intë¡œ ë³€í™˜í•˜ì—¬ TypeError ë°©ì§€
            try:
                x, y, w, h = map(int, bbox)
            except (ValueError, TypeError):
                print(f"âš ï¸ Skipping mask with invalid bbox data: {bbox}")
                continue

            # --- ğŸ› ì—ëŸ¬ í•´ê²° 2: bbox í¬ê¸° ìœ íš¨ì„± ê²€ì‚¬ ---
            # ë„ˆë¹„ë‚˜ ë†’ì´ê°€ 0 ì´í•˜ì¸ ê²½ìš°, crop ì‹œ ì—ëŸ¬ê°€ ë°œìƒí•˜ë¯€ë¡œ ì´ë¥¼ ë°©ì§€
            if w <= 0 or h <= 0:
                print(f"âš ï¸ Skipping mask with invalid bbox dimensions: w={w}, h={h}")
                continue
            
            rect_crop = original_image[y:y+h, x:x+w]
            cropped_mask = mask_ann['segmentation'][y:y+h, x:x+w]
            black_background = np.zeros_like(rect_crop)
            black_background[cropped_mask] = rect_crop[cropped_mask]
            
            if black_background.size == 0: continue

            resized_image = cv2.resize(black_background, (512, 512))
            results = self.cls_model(resized_image, verbose=False)
            r = results[0]

            top1_class_name = self.cls_model.names[r.probs.top1]
            top1_conf = r.probs.top1conf.item()
            
            color = np.random.uniform(0, 255, 3).tolist()

            classifications.append({
                "mask_index": i,
                "top1_class": top1_class_name,
                "confidence": top1_conf,
                "top5_probs": r.probs.top5conf.tolist(),
                "color": color
            })
            
            annotated_image = self._annotate_image(resized_image, r)
            output_filename = f"{top1_class_name}_mask_{i:03d}.png"
            cv2.imwrite(str(timed_output_dir / output_filename), annotated_image)
        
        return classifications

    def _visualize_results(self, original_image: np.ndarray, masks: List[Dict[str, Any]], classifications: List[Dict[str, Any]], image_id: str):
        timed_output_dir = self.output_dir / image_id
        overlay_image = original_image.copy()
        
        # --- âœ¨ ê°œì„ : ì´ë¯¸ì§€ ê²½ê³„ ë³´ì •ì„ ìœ„í•´ ì´ë¯¸ì§€ í¬ê¸° ê°€ì ¸ì˜¤ê¸° ---
        img_h, img_w = overlay_image.shape[:2]

        # --- ğŸ¨ 1ë‹¨ê³„: ëª¨ë“  ë§ˆìŠ¤í¬ ê·¸ë¦¬ê¸° ---
        for i, classification in enumerate(classifications):
            mask = masks[i]['segmentation']
            color = np.array(classification['color'])
            overlay_image[mask] = overlay_image[mask] * 0.4 + color * 0.6
        
        # --- âœï¸ 2ë‹¨ê³„: ì—°ê²°ì„  ë° í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° ---
        for i, classification in enumerate(classifications):
            mask = masks[i]['segmentation']
            bbox = list(map(int, masks[i]['bbox']))
            x, y, w, h = bbox
            color = classification['color']

            # --- ë§ˆìŠ¤í¬ ì¤‘ì‹¬ì  ê³„ì‚° (ì—°ê²°ì„  ëì ) ---
            try:
                M = cv2.moments(mask.astype(np.uint8))
                cX = int(M["m10"] / M["m00"])
                cY = int(M["m01"] / M["m00"])
            except ZeroDivisionError:
                cX, cY = x + w // 2, y + h // 2

            # --- í…ìŠ¤íŠ¸ ì •ë³´ ê³„ì‚° ---
            text = f"{classification['top1_class']} ({classification['confidence']:.2f})"
            font_scale = 0.6
            font_thickness = 1
            (w_text, h_text), _ = cv2.getTextSize(text, cv2.FONT_HERSHEY_SIMPLEX, font_scale, font_thickness)
            
            # --- ğŸ’¡ í•µì‹¬ ê°œì„ : ê²½ê³„ ë³´ì • ë¡œì§ ---
            # 1. ì´ìƒì ì¸ í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (ê¸°ì¡´ ë¡œì§)
            ideal_text_pos_x = x
            ideal_text_pos_y = y - 10 if y > (h_text + 20) else y + h + h_text + 10

            # 2. ë°°ê²½ ì‚¬ê°í˜• ì „ì²´ í¬ê¸° ê³„ì‚° (íŒ¨ë”© í¬í•¨)
            rect_x1 = ideal_text_pos_x - 5
            rect_y1 = ideal_text_pos_y - h_text - 5
            rect_x2 = ideal_text_pos_x + w_text + 5
            rect_y2 = ideal_text_pos_y + 5
            
            # 3. ì´ë¯¸ì§€ ê²½ê³„ë¥¼ ë²—ì–´ë‚˜ëŠ” ì •ë„(offset) ê³„ì‚°
            dx = 0
            if rect_x1 < 0: dx = -rect_x1  # ì™¼ìª½ ì˜¤ë²„í”Œë¡œìš°
            if rect_x2 > img_w: dx = img_w - rect_x2 # ì˜¤ë¥¸ìª½ ì˜¤ë²„í”Œë¡œìš°
            
            dy = 0
            if rect_y1 < 0: dy = -rect_y1 # ìœ„ìª½ ì˜¤ë²„í”Œë¡œìš°
            if rect_y2 > img_h: dy = img_h - rect_y2 # ì•„ë˜ìª½ ì˜¤ë²„í”Œë¡œìš°

            # 4. ê³„ì‚°ëœ offsetìœ¼ë¡œ ëª¨ë“  ê´€ë ¨ ì¢Œí‘œ ë³´ì •
            final_text_pos = (ideal_text_pos_x + dx, ideal_text_pos_y + dy)
            final_rect_pos1 = (rect_x1 + dx, rect_y1 + dy)
            final_rect_pos2 = (rect_x2 + dx, rect_y2 + dy)
            # -----------------------------------------------

            # --- ê·¸ë¦¬ê¸° ---
            # 1. ì—°ê²°ì„  ê·¸ë¦¬ê¸° (ë³´ì •ëœ ìœ„ì¹˜ ê¸°ë°˜)
            line_start_point = (final_text_pos[0] + w_text // 2, final_rect_pos2[1] if final_text_pos[1] > y else final_rect_pos1[1])
            # cv2.line(overlay_image, line_start_point, (cX, cY), (0, 0, 0), 4) # í…Œë‘ë¦¬
            # cv2.line(overlay_image, line_start_point, (cX, cY), (255, 255, 255), 2) # ì¤‘ì‹¬ì„ 
            cv2.line(overlay_image, line_start_point, (cX, cY), (255, 255, 255), 5) # í…Œë‘ë¦¬
            cv2.line(overlay_image, line_start_point, (cX, cY), color, 3) # ì¤‘ì‹¬ì„ 

            # 2. ë°°ê²½ ì‚¬ê°í˜• ê·¸ë¦¬ê¸° (ë³´ì •ëœ ìœ„ì¹˜ ê¸°ë°˜)
            luminance = 0.299 * color[2] + 0.587 * color[1] + 0.114 * color[0]
            text_color = (0, 0, 0) if luminance > 128 else (255, 255, 255)
            cv2.rectangle(overlay_image, final_rect_pos1, final_rect_pos2, color, -1)

            # 3. í…ìŠ¤íŠ¸ ê·¸ë¦¬ê¸° (ë³´ì •ëœ ìœ„ì¹˜ ê¸°ë°˜)
            cv2.putText(overlay_image, text, final_text_pos, cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, font_thickness, cv2.LINE_AA)

        cv2.imwrite(str(timed_output_dir / "_OVERLAY_RESULT.jpg"), overlay_image)
        print(f"âœ… ìµœì¢… ì‹œê°í™” ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {timed_output_dir / '_OVERLAY_RESULT.jpg'}")


    def _annotate_image(self, image: np.ndarray, result) -> np.ndarray:
        annotated_image = image.copy()
        pos_y = 40
        for j, idx in enumerate(result.probs.top5):
            text = f"{j+1}. {self.cls_model.names[int(idx)]}: {result.probs.top5conf[j]:.2f}"
            cv2.putText(annotated_image, text, (20, pos_y), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,0), 3, cv2.LINE_AA)
            cv2.putText(annotated_image, text, (20, pos_y), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255,255,255), 2, cv2.LINE_AA)
            pos_y += 40
        return annotated_image